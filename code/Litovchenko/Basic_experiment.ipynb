{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_experiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9bxbo5tLR3GS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ]
    },
    {
      "metadata": {
        "id": "bNUfQlyZR1o3",
        "colab_type": "code",
        "outputId": "60325886-0d5d-401a-f460-6e104bac1a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Ww59NHhSC3E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load, split and preprocess the dataset"
      ]
    },
    {
      "metadata": {
        "id": "qJkMG3eeR8y_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_tv, y_tv) = mnist.load_data()\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_tv, y_tv, \n",
        "                                                test_size=0.5, random_state=42)\n",
        "\n",
        "rows, cols = 28, 28\n",
        "num_classes = 10\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], rows, cols, 1)\n",
        "x_val = x_val.reshape(x_val.shape[0], rows, cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], rows, cols, 1)\n",
        "input_shape = (rows, cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_val = x_val.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yuc40KVXegA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define F1 score"
      ]
    },
    {
      "metadata": {
        "id": "BdHdqSagefWe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    \n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrNa8919ShRZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Define basic CNN"
      ]
    },
    {
      "metadata": {
        "id": "0ZMxE90-SYrV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy', f1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZvxCX-iGd9d7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define F1 score"
      ]
    },
    {
      "metadata": {
        "id": "oQ2RFjL-eA2T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Syt_ueGSphA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Training process"
      ]
    },
    {
      "metadata": {
        "id": "JgIqsayRSk8n",
        "colab_type": "code",
        "outputId": "5de294f4-7ac6-465b-f2ca-8ee5bf731d23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1890
        }
      },
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_acc', mode='max', patience=5, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('best.model', monitor='val_acc', mode='max', save_best_only=True, verbose=1)\n",
        "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.7, patience=3, verbose=1)\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=512,\n",
        "          epochs=100,\n",
        "          callbacks=[early_stopping, model_checkpoint, reduce_lr_on_plateau],\n",
        "          verbose=1,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 5000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 13s 222us/step - loss: 0.8217 - acc: 0.8036 - f1: 0.7955 - val_loss: 0.1547 - val_acc: 0.9506 - val_f1: 0.9509\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.95060, saving model to best.model\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.1395 - acc: 0.9580 - f1: 0.9581 - val_loss: 0.1182 - val_acc: 0.9622 - val_f1: 0.9623\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.95060 to 0.96220, saving model to best.model\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0976 - acc: 0.9707 - f1: 0.9708 - val_loss: 0.1166 - val_acc: 0.9676 - val_f1: 0.9680\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.96220 to 0.96760, saving model to best.model\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0748 - acc: 0.9774 - f1: 0.9777 - val_loss: 0.0318 - val_acc: 0.9908 - val_f1: 0.9910\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.96760 to 0.99080, saving model to best.model\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0616 - acc: 0.9816 - f1: 0.9817 - val_loss: 0.0364 - val_acc: 0.9890 - val_f1: 0.9890\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.99080\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0563 - acc: 0.9835 - f1: 0.9836 - val_loss: 0.0283 - val_acc: 0.9918 - val_f1: 0.9917\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.99080 to 0.99180, saving model to best.model\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0489 - acc: 0.9852 - f1: 0.9854 - val_loss: 0.0240 - val_acc: 0.9918 - val_f1: 0.9920\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.99180\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0447 - acc: 0.9862 - f1: 0.9863 - val_loss: 0.0538 - val_acc: 0.9850 - val_f1: 0.9848\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.99180\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0436 - acc: 0.9865 - f1: 0.9865 - val_loss: 0.0222 - val_acc: 0.9926 - val_f1: 0.9928\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.99180 to 0.99260, saving model to best.model\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0380 - acc: 0.9880 - f1: 0.9881 - val_loss: 0.0264 - val_acc: 0.9930 - val_f1: 0.9928\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.99260 to 0.99300, saving model to best.model\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0359 - acc: 0.9886 - f1: 0.9887 - val_loss: 0.0371 - val_acc: 0.9890 - val_f1: 0.9893\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.99300\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0348 - acc: 0.9889 - f1: 0.9890 - val_loss: 0.0309 - val_acc: 0.9920 - val_f1: 0.9919\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99300\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0323 - acc: 0.9898 - f1: 0.9898 - val_loss: 0.0313 - val_acc: 0.9918 - val_f1: 0.9920\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99300\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.7.\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0280 - acc: 0.9914 - f1: 0.9914 - val_loss: 0.0220 - val_acc: 0.9932 - val_f1: 0.9934\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.99300 to 0.99320, saving model to best.model\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0266 - acc: 0.9920 - f1: 0.9919 - val_loss: 0.0195 - val_acc: 0.9946 - val_f1: 0.9949\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.99320 to 0.99460, saving model to best.model\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0258 - acc: 0.9921 - f1: 0.9922 - val_loss: 0.0189 - val_acc: 0.9934 - val_f1: 0.9934\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99460\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0250 - acc: 0.9923 - f1: 0.9925 - val_loss: 0.0194 - val_acc: 0.9932 - val_f1: 0.9933\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99460\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0235 - acc: 0.9928 - f1: 0.9927 - val_loss: 0.0192 - val_acc: 0.9946 - val_f1: 0.9948\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99460\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.4899999916553497.\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0226 - acc: 0.9929 - f1: 0.9930 - val_loss: 0.0186 - val_acc: 0.9948 - val_f1: 0.9949\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99460 to 0.99480, saving model to best.model\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0191 - acc: 0.9944 - f1: 0.9943 - val_loss: 0.0169 - val_acc: 0.9952 - val_f1: 0.9953\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.99480 to 0.99520, saving model to best.model\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0191 - acc: 0.9940 - f1: 0.9940 - val_loss: 0.0171 - val_acc: 0.9952 - val_f1: 0.9952\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99520\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0190 - acc: 0.9940 - f1: 0.9941 - val_loss: 0.0174 - val_acc: 0.9940 - val_f1: 0.9941\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99520\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0174 - acc: 0.9942 - f1: 0.9943 - val_loss: 0.0193 - val_acc: 0.9942 - val_f1: 0.9938\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99520\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.3429999858140945.\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0182 - acc: 0.9942 - f1: 0.9942 - val_loss: 0.0176 - val_acc: 0.9950 - val_f1: 0.9951\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99520\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0159 - acc: 0.9949 - f1: 0.9949 - val_loss: 0.0197 - val_acc: 0.9944 - val_f1: 0.9944\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99520\n",
            "Epoch 00025: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa8b589dd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "02dOMSPPS62A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Results"
      ]
    },
    {
      "metadata": {
        "id": "g3vL4lJ-SxqZ",
        "colab_type": "code",
        "outputId": "026ea195-2523-47a9-aedf-51a3e572302d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "model = load_model('best.model', custom_objects={'f1': f1})\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]: 0.5f}')\n",
        "print(f'Test accuracy:{score[1]: 0.5f}')\n",
        "print(f'Test f1_score:{score[2]: 0.5f}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss:  0.01795\n",
            "Test accuracy: 0.99380\n",
            "Test f1_score: 0.99380\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_cPHRsyYeVTs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}