{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_train_skel_features = None, None, None\n",
    "with open(\"data/train_info\", \"rb\") as fin:\n",
    "    data = pickle.load(fin)\n",
    "    X_train, y_train, X_train_skel_features = data[\"data\"], data[\"labels\"], data[\"skel_features\"]\n",
    "\n",
    "X_test, y_test, X_test_skel_features = None, None, None\n",
    "with open(\"data/test_info\", \"rb\") as fin:\n",
    "    data = pickle.load(fin)\n",
    "    X_test, y_test, X_test_skel_features = data[\"data\"], data[\"labels\"], data[\"skel_features\"]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy', f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_acc', mode='max', patience=5, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.5, patience=3, verbose=1)\n",
    "callbacks=[early_stopping, model_checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10000\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 1.6019 - acc: 0.4261 - f1: 0.3232 - val_loss: 0.5978 - val_acc: 0.8435 - val_f1: 0.7671\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84350, saving model to best_model.h5\n",
      "Epoch 2/10000\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.7594 - acc: 0.7422 - f1: 0.7384 - val_loss: 0.2755 - val_acc: 0.9221 - val_f1: 0.9200\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.84350 to 0.92210, saving model to best_model.h5\n",
      "Epoch 3/10000\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.5561 - acc: 0.8179 - f1: 0.8175 - val_loss: 0.1733 - val_acc: 0.9481 - val_f1: 0.9506\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.92210 to 0.94810, saving model to best_model.h5\n",
      "Epoch 4/10000\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.4561 - acc: 0.8550 - f1: 0.8554 - val_loss: 0.1557 - val_acc: 0.9524 - val_f1: 0.9530\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.94810 to 0.95240, saving model to best_model.h5\n",
      "Epoch 5/10000\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.3972 - acc: 0.8762 - f1: 0.8780 - val_loss: 0.1747 - val_acc: 0.9494 - val_f1: 0.9499\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.95240\n",
      "Epoch 6/10000\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.3484 - acc: 0.8923 - f1: 0.8938 - val_loss: 0.1499 - val_acc: 0.9563 - val_f1: 0.9571\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.95240 to 0.95630, saving model to best_model.h5\n",
      "Epoch 7/10000\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.3194 - acc: 0.9015 - f1: 0.9024 - val_loss: 0.1047 - val_acc: 0.9670 - val_f1: 0.9678\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.95630 to 0.96700, saving model to best_model.h5\n",
      "Epoch 8/10000\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.3022 - acc: 0.9072 - f1: 0.9084 - val_loss: 0.1170 - val_acc: 0.9669 - val_f1: 0.9668\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.96700\n",
      "Epoch 9/10000\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2868 - acc: 0.9125 - f1: 0.9139 - val_loss: 0.0854 - val_acc: 0.9744 - val_f1: 0.9748\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.96700 to 0.97440, saving model to best_model.h5\n",
      "Epoch 10/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2713 - acc: 0.9176 - f1: 0.9175 - val_loss: 0.0905 - val_acc: 0.9722 - val_f1: 0.9729\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97440\n",
      "Epoch 11/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2639 - acc: 0.9215 - f1: 0.9213 - val_loss: 0.0985 - val_acc: 0.9714 - val_f1: 0.9711\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.97440\n",
      "Epoch 12/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2533 - acc: 0.9224 - f1: 0.9227 - val_loss: 0.0977 - val_acc: 0.9727 - val_f1: 0.9722\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.97440\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.5.\n",
      "Epoch 13/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2411 - acc: 0.9285 - f1: 0.9286 - val_loss: 0.0758 - val_acc: 0.9762 - val_f1: 0.9768\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.97440 to 0.97620, saving model to best_model.h5\n",
      "Epoch 14/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2339 - acc: 0.9296 - f1: 0.9310 - val_loss: 0.0744 - val_acc: 0.9770 - val_f1: 0.9774\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.97620 to 0.97700, saving model to best_model.h5\n",
      "Epoch 15/10000\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2277 - acc: 0.9323 - f1: 0.9324 - val_loss: 0.0729 - val_acc: 0.9776 - val_f1: 0.9777\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.97700 to 0.97760, saving model to best_model.h5\n",
      "Epoch 16/10000\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2216 - acc: 0.9339 - f1: 0.9336 - val_loss: 0.0678 - val_acc: 0.9782 - val_f1: 0.9782\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.97760 to 0.97820, saving model to best_model.h5\n",
      "Epoch 17/10000\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2248 - acc: 0.9326 - f1: 0.9327 - val_loss: 0.0696 - val_acc: 0.9786 - val_f1: 0.9787\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.97820 to 0.97860, saving model to best_model.h5\n",
      "Epoch 18/10000\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2216 - acc: 0.9333 - f1: 0.9338 - val_loss: 0.0693 - val_acc: 0.9787 - val_f1: 0.9789\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.97860 to 0.97870, saving model to best_model.h5\n",
      "Epoch 19/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2152 - acc: 0.9365 - f1: 0.9365 - val_loss: 0.0669 - val_acc: 0.9792 - val_f1: 0.9794\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.97870 to 0.97920, saving model to best_model.h5\n",
      "Epoch 20/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2143 - acc: 0.9358 - f1: 0.9360 - val_loss: 0.0673 - val_acc: 0.9797 - val_f1: 0.9801\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.97920 to 0.97970, saving model to best_model.h5\n",
      "Epoch 21/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2104 - acc: 0.9365 - f1: 0.9375 - val_loss: 0.0655 - val_acc: 0.9798 - val_f1: 0.9799\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.97970 to 0.97980, saving model to best_model.h5\n",
      "Epoch 22/10000\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2094 - acc: 0.9369 - f1: 0.9375 - val_loss: 0.0618 - val_acc: 0.9804 - val_f1: 0.9809\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.97980 to 0.98040, saving model to best_model.h5\n",
      "Epoch 23/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2059 - acc: 0.9384 - f1: 0.9393 - val_loss: 0.0636 - val_acc: 0.9814 - val_f1: 0.9809\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.98040 to 0.98140, saving model to best_model.h5\n",
      "Epoch 24/10000\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2032 - acc: 0.9384 - f1: 0.9398 - val_loss: 0.0638 - val_acc: 0.9800 - val_f1: 0.9798\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.98140\n",
      "Epoch 25/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2013 - acc: 0.9399 - f1: 0.9400 - val_loss: 0.0625 - val_acc: 0.9799 - val_f1: 0.9801\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.98140\n",
      "Epoch 26/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2006 - acc: 0.9407 - f1: 0.9411 - val_loss: 0.0597 - val_acc: 0.9807 - val_f1: 0.9812\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.98140\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.25.\n",
      "Epoch 27/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1999 - acc: 0.9408 - f1: 0.9413 - val_loss: 0.0602 - val_acc: 0.9821 - val_f1: 0.9819\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.98140 to 0.98210, saving model to best_model.h5\n",
      "Epoch 28/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1933 - acc: 0.9417 - f1: 0.9426 - val_loss: 0.0596 - val_acc: 0.9813 - val_f1: 0.9816\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.98210\n",
      "Epoch 29/10000\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1962 - acc: 0.9408 - f1: 0.9418 - val_loss: 0.0597 - val_acc: 0.9809 - val_f1: 0.9815\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.98210\n",
      "Epoch 30/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1959 - acc: 0.9409 - f1: 0.9419 - val_loss: 0.0591 - val_acc: 0.9816 - val_f1: 0.9821\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.98210\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.125.\n",
      "Epoch 31/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1893 - acc: 0.9431 - f1: 0.9435 - val_loss: 0.0576 - val_acc: 0.9822 - val_f1: 0.9817\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.98210 to 0.98220, saving model to best_model.h5\n",
      "Epoch 32/10000\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1903 - acc: 0.9423 - f1: 0.9424 - val_loss: 0.0570 - val_acc: 0.9822 - val_f1: 0.9822\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.98220\n",
      "Epoch 33/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1897 - acc: 0.9425 - f1: 0.9435 - val_loss: 0.0568 - val_acc: 0.9818 - val_f1: 0.9819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_acc did not improve from 0.98220\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0625.\n",
      "Epoch 34/10000\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.1907 - acc: 0.9431 - f1: 0.9435 - val_loss: 0.0566 - val_acc: 0.9816 - val_f1: 0.9821\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.98220\n",
      "Epoch 35/10000\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1885 - acc: 0.9422 - f1: 0.9431 - val_loss: 0.0566 - val_acc: 0.9820 - val_f1: 0.9822\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.98220\n",
      "Epoch 36/10000\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.1894 - acc: 0.9430 - f1: 0.9442 - val_loss: 0.0568 - val_acc: 0.9821 - val_f1: 0.9825\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.98220\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.03125.\n",
      "Epoch 00036: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f184a4f1358>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10000,\n",
    "          callbacks=callbacks,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.05756415659706108\n",
      "Test accuracy: 0.9822\n",
      "Test f1_score: 0.9816322724342346\n"
     ]
    }
   ],
   "source": [
    "model = load_model('best_model.h5', custom_objects={'f1': f1})\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test f1_score:', score[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
