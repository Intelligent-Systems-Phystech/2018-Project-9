{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пробная модель по граф эмбеддингу и последующей классификации\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"train_info\", \"rb\") as fin:\n",
    "    data = pickle.load(fin)\n",
    "    X_train, y_train, X_train_skel_features = data[\"data\"], data[\"labels\"], data[\"skel_features\"]\n",
    "\n",
    "with open(\"test_info\", \"rb\") as fin:\n",
    "    data = pickle.load(fin)\n",
    "    X_test, y_test, X_test_skel_features = data[\"data\"], data[\"labels\"], data[\"skel_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 1108, 3: 1052, 7: 1038, 6: 1011, 2: 988, 0: 983, 9: 971, 4: 963, 5: 953, 8: 933})\n",
      "Counter({1: 123, 7: 107, 2: 106, 6: 104, 5: 99, 9: 98, 3: 93, 8: 93, 0: 92, 4: 85})\n"
     ]
    }
   ],
   "source": [
    "#немного уменьшаем корпуса\n",
    "train_size = 10000\n",
    "test_size = 1000\n",
    "import numpy, collections\n",
    "def unison_shuffled_copies(a, b, size):\n",
    "    assert len(a) == len(b)\n",
    "    p = numpy.random.permutation(len(a))\n",
    "    return a[p][:size], b[p][:size]\n",
    "\n",
    "y_train, X_train_skel_features = unison_shuffled_copies(np.asarray(y_train), np.asarray(X_train_skel_features), train_size)\n",
    "y_test, X_test_skel_features = unison_shuffled_copies(np.asarray(y_test), np.asarray(X_test_skel_features), test_size)\n",
    "print(collections.Counter(y_train))\n",
    "print(collections.Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_edges_ged_rad(skel_features):\n",
    "    edges_x = []\n",
    "    edges_y = []\n",
    "    degs = []\n",
    "    rads = []\n",
    "    for i, d in enumerate(skel_features):\n",
    "        if i % 4 == 0:\n",
    "            edges_x.append(d)\n",
    "        if i % 4 == 1:\n",
    "            edges_y.append(d)\n",
    "        if i % 4 == 2:\n",
    "            #degs.append([[edges_x[-1], edges_y[-1]], d])\n",
    "            deg = d\n",
    "        if i % 4 == 3:\n",
    "            #rads.append([[edges_x[-1], edges_y[-1]], d])\n",
    "            rad = d\n",
    "            degs.append([(edges_x[-1], edges_y[-1]), deg, rad])\n",
    "    return degs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# достаем признаки скелетов\n",
    "skel_feats = []\n",
    "for skel in X_train_skel_features:\n",
    "    skel_feats.append(my_get_edges_ged_rad(skel))\n",
    "    \n",
    "skel_feats_test = []\n",
    "for skel in X_test_skel_features:\n",
    "    skel_feats_test.append(my_get_edges_ged_rad(skel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в качестве вершин графа в постановке Node2Vec должны быть строки, поэтому индексируем вершины,\n",
    "# заданные своими координатами. Для индексирования сортируем вершины по высоте (координате y)\n",
    "node_dicts = []\n",
    "node_dicts_test = []\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "\n",
    "for skel_feat in skel_feats:\n",
    "    node_dict = {}\n",
    "    nodes = [t[0] for t in skel_feat]\n",
    "    for i,j in enumerate(sorted(list(set(nodes)), key=takeSecond)):\n",
    "        node_dict[j] = str(i)\n",
    "    node_dicts.append(node_dict)\n",
    "    \n",
    "for skel_feat in skel_feats_test:\n",
    "    node_dict = {}\n",
    "    nodes = [t[0] for t in skel_feat]\n",
    "    for i,j in enumerate(sorted(list(set(nodes)), key=takeSecond)):\n",
    "        node_dict[j] = str(i)\n",
    "    node_dicts_test.append(node_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#конструируем графы в представлении NetrworkX\n",
    "graphs = []\n",
    "for j in range(len(skel_feats)):\n",
    "    g = nx.Graph()\n",
    "    for i in range(len(skel_feats[j])):\n",
    "        if i % 2 == 0:\n",
    "        #att = {'deg':degs_[i][1]}\n",
    "            g.add_edge(u_of_edge=node_dicts[j][skel_feats[j][i][0]], v_of_edge=node_dicts[j][skel_feats[j][i+1][0]], weight=np.sqrt((skel_feats[j][i][0][0])**2+(skel_feats[j][i][0][1])**2))\n",
    "    graphs.append(g)\n",
    "    \n",
    "graphs_test = []\n",
    "for j in range(len(skel_feats_test)):\n",
    "    g = nx.Graph()\n",
    "    for i in range(len(skel_feats_test[j])):\n",
    "        if i % 2 == 0:\n",
    "        #att = {'deg':degs_[i][1]}\n",
    "            g.add_edge(u_of_edge=node_dicts_test[j][skel_feats_test[j][i][0]], v_of_edge=node_dicts_test[j][skel_feats_test[j][i+1][0]], weight=np.sqrt((skel_feats_test[j][i][0][0])**2+(skel_feats_test[j][i][0][1])**2))\n",
    "    graphs_test.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(graphs), len(graphs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "#обучаем эмбеддинги графов из обучающей выборки\n",
    "embedding_matrixes = []\n",
    "for i in range(len(graphs)):\n",
    "# Precompute probabilities and generate walks\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    node2vec = Node2Vec(graph=graphs[i], dimensions=10, walk_length=15,\n",
    "                        num_walks=10, workers=4, weight_key='weight', quiet=True)\n",
    "\n",
    "# Embed nodes\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `diemnsions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "\n",
    "    embedding_matrix = np.array([])\n",
    "    for t in range(len(node_dicts[i])):    \n",
    "        embedding_matrix = np.append(embedding_matrix, np.asarray(model.wv[str(t)]))\n",
    "    del model\n",
    "    embedding_matrixes.append(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_matrixes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "#обучаем эмбеддинги графов из обучающей выборки\n",
    "embedding_matrixes_test = []\n",
    "for i in range(len(graphs_test)):\n",
    "# Precompute probabilities and generate walks\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    node2vec = Node2Vec(graph=graphs_test[i], dimensions=10, walk_length=15,\n",
    "                        num_walks=20, workers=4, weight_key='weight', quiet=True)\n",
    "\n",
    "# Embed nodes\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `diemnsions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "\n",
    "    embedding_matrix = np.array([])\n",
    "    for t in range(len(node_dicts_test[i])):    \n",
    "        embedding_matrix = np.append(embedding_matrix, np.asarray(model.wv[str(t)]))\n",
    "    del model\n",
    "    embedding_matrixes_test.append(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "sizes = np.asarray([len(d) for d in node_dicts])\n",
    "max_size = 10*sizes.max()\n",
    "print(max_size)\n",
    "\n",
    "sizes_test = np.asarray([len(d) for d in node_dicts_test])\n",
    "max_size_test = 10*sizes.max()\n",
    "print(max_size_test)\n",
    "assert max_size < 700 and max_size_test < 700\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#приводим все описания графов к константной длине\n",
    "for i in range(len(embedding_matrixes)):\n",
    "    \n",
    "    if len(embedding_matrixes[i]) < max_size:\n",
    "        embedding_matrixes[i] = np.append(embedding_matrixes[i], np.zeros(max_size - len(embedding_matrixes[i])))\n",
    "for i in range(len(embedding_matrixes_test)):\n",
    "    \n",
    "    if len(embedding_matrixes_test[i]) < max_size:\n",
    "        embedding_matrixes_test[i] = np.append(embedding_matrixes_test[i], np.zeros(max_size - len(embedding_matrixes_test[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 600\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_matrixes[7]), len(embedding_matrixes_test[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# описываем перцептрон\n",
    "import tensorflow as tf\n",
    "n_hidden = 64\n",
    "xavier_init = tf.contrib.layers.xavier_initializer(seed=12)\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(xavier_init([max_size, n_hidden])),\n",
    "    'h2': tf.Variable(xavier_init([n_hidden, n_hidden])),\n",
    "    'h3': tf.Variable(xavier_init([n_hidden, n_hidden])),\n",
    "    'h4': tf.Variable(xavier_init([n_hidden, 10]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([n_hidden])),\n",
    "    'b2': tf.Variable(tf.zeros([n_hidden])),\n",
    "    'b3': tf.Variable(tf.zeros([n_hidden])),\n",
    "    'b4': tf.Variable(tf.zeros([10]))\n",
    "}\n",
    "\n",
    "def neural_net(input):\n",
    "    output = tf.nn.leaky_relu(tf.add(tf.matmul(input, weights['h1']), biases['b1']))\n",
    "    output = tf.nn.leaky_relu(tf.add(tf.matmul(output, weights['h2']), biases['b2']))\n",
    "    output = tf.nn.leaky_relu(tf.add(tf.matmul(output, weights['h3']), biases['b3']))\n",
    "    output = tf.add(tf.matmul(output, weights['h4']), biases['b4'])\n",
    "    return output\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, max_size])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "pred = tf.nn.softmax(logits)\n",
    "lab = tf.argmax(pred, 1)\n",
    "# Apply softmax to logits\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3663654\n",
      "2.0915818\n",
      "1.9650061\n",
      "2.032762\n",
      "1.7501487\n",
      "1.8280786\n",
      "1.8860668\n",
      "1.8505545\n",
      "1.8155171\n",
      "1.9980762\n",
      "F1 classification score {0: 0.3537414965986394, 1: 0.71356783919598, 2: 0.0, 3: 0.11650485436893203, 4: 0.023255813953488372, 5: 0.2626582278481012, 6: 0.0, 7: 0.3922829581993569, 8: 0.2, 9: 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boeing/.local/venvs/tfenv/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def one_hot_y(a):\n",
    "    out =[]\n",
    "    for c in a:\n",
    "        result = np.zeros(10, dtype = 'int')\n",
    "        result[c] = 1\n",
    "        out.append(result)\n",
    "    return out\n",
    "\n",
    "batch_size = 100\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #\n",
    "    for i in range(int(len(y_train)/batch_size)):\n",
    "        loss, _ = sess.run((loss_op, train_op), feed_dict = {X:embedding_matrixes[i*batch_size:(i+1)*batch_size], Y:one_hot_y(y_train[i*batch_size:(i+1)*batch_size])})\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(loss)\n",
    "    accs = np.array([])\n",
    "    learned_labels_ = np.array([], dtype = 'int')\n",
    "    for i in range(int(len(y_test)/batch_size)):\n",
    "        #acc = sess.run(accuracy, feed_dict = {X:embedding_matrixes_test[i*batch_size:(i+1)*batch_size], Y:one_hot_y(y_test[i*batch_size:(i+1)*batch_size])})\n",
    "        #accs = np.append(accs, acc)\n",
    "   #print(np.mean(accs))\n",
    "        learned_labels = sess.run(lab, feed_dict = {X:embedding_matrixes_test[i*batch_size:(i+1)*batch_size], Y:one_hot_y(y_test[i*batch_size:(i+1)*batch_size])})\n",
    "        #print(len(learned_labels))\n",
    "        learned_labels_ = np.append(learned_labels_, learned_labels)\n",
    "    #print(learned_labels_)\n",
    "    score = f1_score(y_true=y_test,y_pred=learned_labels_, average=None)\n",
    "    f1_dict = dict(zip([0,1,2,3,4,5,6,7,8,9], score))\n",
    "    print('F1 classification score',f1_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# как мы видим, модель получилась неадекватной :()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
